{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get to know with HuggingFace GPT-2\n",
    "\n",
    "This notebook contains test instructions that will be placed inside train.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 687kB/s]\n",
      "c:\\Users\\barto\\miniconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\barto\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.46MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 664kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 548M/548M [07:42<00:00, 1.18MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 124kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie is about a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"The movie is about\"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Get the predicted next sub-word\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./model/gpt2_movie_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parmeters in GPT-2 model: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parmeters in GPT-2 model: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "# model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8248, 6176]], device='cuda:0')\n",
      "50256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8248,  6176,     1,   550, 14869,   422, 15589,   338,  2656,  1621,\n",
       "            11,   366,   464,  4586, 16147,     1,   284,   366, 27676, 15339,\n",
       "           290, 10598,   553,  1642,   262,  2551,  2562,    13,  2935, 32968,\n",
       "           416,  6240, 20320,   371,    13,   775,   600,   430,   549,   355,\n",
       "           366, 10262,  1758,   262,  2851,   553,  2935, 26674,   274,   318,\n",
       "           783,   262,  1266,    12,  4002,  4014,   290, 19466,  4014,   508,\n",
       "         13831,   257,  4334,  2756,   329,   262,  1943,   286,   465,  1492,\n",
       "          8670,  1505,  6176,   357, 11528,   828,   543, 14999,  1657,   319,\n",
       "           262,   779,   290, 22036,   286, 18423,  5010,   287,   428,  6980,\n",
       "            13,   198,   198, 29011,    11,  1810,   373, 11791,   416,   867,\n",
       "          2458,    13, 12168,  8581, 15434,  8793, 12411,  8088,  1390,  1266,\n",
       "          4286,    11,  1266,  8674,    11,  1266,  8674,  4286,    11,   290,\n",
       "          1266,  1492,    13,   679,   635,  6492,  4708]], device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "text = tokenizer(\"Star Wars\", return_tensors='pt')\n",
    "tokens = text['input_ids'].to('cuda')\n",
    "pred = torch.tensor(-1)\n",
    "print(tokens)\n",
    "eof = tokenizer.encode('<|endoftext|>', return_tensors='pt').item()\n",
    "print(eof)\n",
    "\n",
    "\n",
    "while len(tokens[0]) != 128:\n",
    "    n_inp = tokens\n",
    "    output = model(n_inp)\n",
    "    probs = output[0].softmax(dim=-1)\n",
    "    pred = torch.multinomial(probs[0, -1, :], 1)\n",
    "    tokens = torch.cat([n_inp, pred.view(1,1)], dim=-1)\n",
    "    if pred.item() == eof:\n",
    "        break\n",
    "n_inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars\" had shifted from 1977's original story, \"The Last Jedi\" to \"Garuda and Rome,\" making the decision easy. Described by professor Nicholas R. Weintraub as \"Agape the Dragon,\" Descartes is now the best-known critic and libertarian critic who pays a heavy price for the success of his book Opium Wars (2008), which shed light on the use and monopoly of pharmaceutical drugs in this era.\n",
      "\n",
      "Nevertheless, War was accompanied by many changes. Several Academy Awards achieved notable reviews including best picture, best actor, best actor picture, and best book. He also obtained professional\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(n_inp.view(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that GPT-2 is producing some rich sentences but it's nowhere near the target of shortly describing the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50256])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|endoftext|>\",return_tensors='pt').view(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
